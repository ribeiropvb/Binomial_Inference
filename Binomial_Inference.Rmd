---
title: "Binomial Inference"
author: "Pedro Victor Brasil Ribeiro"
date: "2021-12-19 - Last changed in `r Sys.Date()`"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{bm}
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  echo = F
  , include = F
)
```

```{r}
set.seed(18122021)
data <- rbinom(275, 17, 0.75)

m <- length(data)
media <- mean(data)
```

```{r}
vero <- function(x){
  p <- x[1]
  n <- x[2]
  -(
    m*log(gamma(n + 1)) - sum(log(gamma(n - data + 1))) - sum(log(gamma(data + 1)))+
      m*media*log(p) + m*n*log(1 - p) - m*media*log(1 - p)
  )
}
```

```{r}
## Newton-Raphson

newton.raphson <- function(f, a, b, tol = 1e-5, n = 1000) {
  require(numDeriv) # Package for computing f'(x)
  
  x0 <- a # Set start value to supplied lower bound
  k <- n # Initialize for iteration results
  
  # Check the upper and lower bounds to see if approximations result in 0
  fa <- f(a)
  if (fa == 0.0) {
    return(a)
  }
  
  fb <- f(b)
  if (fb == 0.0) {
    return(b)
  }
  
  for (i in 1:n) {
    dx <- genD(func = f, x = x0)$D[1] # First-order derivative f'(x0)
    x1 <- x0 - (f(x0) / dx) # Calculate next value x1
    k[i] <- x1 # Store x1
    # Once the difference between x0 and x1 becomes sufficiently small, output the results.
    if (abs(x1 - x0) < tol) {
      root.approx <- tail(k, n=1)
      res <- list('root approximation' = root.approx, 'iterations' = k)
      return(res)
    }
    # If Newton-Raphson has not yet reached convergence set x1 as x0 and continue
    x0 <- x1
  }
  print('Too many iterations in method')
}
```

```{r}
quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
}
```

```{r}
rbinom(25, 10, 0.15)
```

Supose you have a sample that comes from a binomial distribution, and the sample size is igual to m. How can you estimate the parameter p and n With only that sample?

To solve this problem we use a estimator deriving from the likelihood function, which is define as:

\begin{equation}
  L(x; \theta) = \prod_{i = 1}^n f(x_i; \theta) \label{lik}
\end{equation}

And then we find the maximum point of the function, or in other words, the point that make the derivative of $f(x; \theta)$ igual to 0.

# The Problem

Let $X_1, X_2, \cdots, X_m$ be a random variable i.i.d. (Independent and identically distributed) a binomial sample of m observations with parameters n and p, in other words $X \sim Bin(n,p)$. Then we know that the density function of a binomial is:

\begin{equation}
  f(x; \{n, p\}) = \binom{n}{x} p^x (1 - p)^{n - x}; \qquad \text{$n \in \mathbb{Z}^* \quad p \in (0,1)$} \label{binom}
\end{equation}

So first of all we need to find the likehood function for a binomial distribution so using the equation \ref{lik}, where $f(x; \theta)$ for $\theta = \{ n, p \}$, expressed in the equation \ref{binom}.

## Estimation

Usually is more convenient to work with the derivative of $log[(L(x; \theta)]$, note that the maximum point of $L(x; \theta)$ and $log[L(x; \theta)]$ is the same point. Since we are looking at the maximum point then we know that $L'(x; \theta) = 0$, so:

\begin{align*}
  log'[L(x; \theta)] =& \frac{\partial log[L(x; \theta)]}{\partial \theta}  \\
  =& \frac{1}{L(x; \theta)} \frac{\partial L(x; \theta)}{\partial \theta}; \quad \text{but we know that in the maximum point} \frac{\partial L(x; \theta)}{\partial \theta} = 0 \\
  \Rightarrow \frac{\partial log[L(x; \theta)]}{\partial \theta} =& 0 
\end{align*}

$\forall x_0 \quad$That $f(x_0; \theta) \neq 0$.

\begin{align*}
  L(x; \theta) =& \prod_{i = 1}^n f(x_i; \theta) \\
  =& \prod_{i = 1}^n \binom{n}{x_i} p^{x_i} (1 - p)^{n - x_i} \\
  \Rightarrow l(x; \theta) =& \sum_{i = 1}^n log \Big{[} \binom{n}{x_i} \Big{]} + \sum_{i = 1} x_i log(p) + \sum_{i = 1}^n (n - x_i) log(1 - p)
\end{align*}

## Estimate p, with n known


```{r}
# Estimar p => n Conhecido

n <- 17
frp <- function(x) 1/print(x)*media - (n - media)/(1-x)

p_hat <- newton.raphson(frp, 0.001, 1) %>% 
  quiet()
p_hat

# param_p <- matrix(NA, ncol = 2, nrow = 10000)
# for (i in 1:10000) {
#   set.seed(18122021-i)
#   data <- rbinom(275, 17, 0.75)
# 
#   m <- length(data)
#   media <- mean(data)
# 
#   val <- newton.raphson(frp, 0.001, 1) %>% 
#   quiet() %>% 
#     .$`root approximation`
#   param_p[i,] <- cbind(i, val)
# }
# 
# param_p[,2] %>%
#   round(4) %>%
#   as_tibble() %>%
#   mutate(ID = 1:length(value)) %>%
#   ggplot(aes(x = ID, y = value))+
#   geom_point()+
#   geom_hline(yintercept = 0.75, color = 'red')+
#   theme_classic()
```


## Estimate n, with p know

```{r}
# Estimar n => p Conhecido

set.seed(18122021)
p <- 0.15
data <- rbinom(275, 17, p)
m <- length(data)
media <- mean(data)

frn <- function(x) m*psigamma((x+1),0) - sum(psigamma((x - data + 1))) + m*log(1-p)

n_hat <- newton.raphson(frn,max(data),100) %>% 
  quiet()
n_hat$`root approximation`
```


## Both n and p unknow

```{r}
# n e p desconhecidos
## Maximizar verossimilhan√ßa

set.seed(18122021)
data <- rbinom(275, 17, 0.75)

m <- length(data)
media <- mean(data)

optim(
  fn = vero
  , par = c(0.01,(max(data)+1))
  , lower = c(0.001,max(data))
  , upper = c(0.999,100)
  , method = 'L-BFGS-B'
  , hessian = T
)
```

```{r}
# param <- matrix(NA, ncol = 2, nrow = 10000)
# for (i in 1:10000) {
#   data <- rbinom(275, 17, 0.75)
#   
#   m <- length(data)
#   media <- mean(data)
#   
#   val <- optim(
#     fn = vero
#     , par = c(0.01,(max(data)+1))
#     , lower = c(0.001,max(data))
#     , upper = c(0.999,100)
#     , method = 'L-BFGS-B'
#     , hessian = T
#   )
#   param[i,] <- val$par
# }
# 
# param[,2] %>%
#   as_tibble() %>% 
#   mutate(ID = 1:length(value)) %>% 
#   ggplot(aes(x = ID, y = value))+
#   geom_point()+
#   geom_hline(yintercept = 17, color = 'red')+
#   theme_classic()
# 
# param[,1] %>%
#   round(4) %>% 
#   as_tibble() %>% 
#   mutate(ID = 1:length(value)) %>% 
#   ggplot(aes(x = ID, y = value))+
#   geom_point()+
#   geom_hline(yintercept = 0.75, color = 'red')+
#   theme_classic()
```
