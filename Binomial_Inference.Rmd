---
title: "Binomial Inference"
author: "Pedro Victor Brasil Ribeiro"
date: "2021-12-19 - Last changed in `r Sys.Date()`"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{bm}
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(magick)
knitr::opts_chunk$set(
  echo = F
  , include = F
)
```

```{r}
set.seed(18122021)
data <- rbinom(275, 17, 0.75)

m <- length(data)
media <- mean(data)
```

```{r}
vero <- function(x){
  p <- x[1]
  n <- x[2]
  -(
    m*log(gamma(n + 1)) - sum(log(gamma(n - data + 1))) - sum(log(gamma(data + 1)))+
      m*media*log(p) + m*n*log(1 - p) - m*media*log(1 - p)
  )
}
```

```{r}
quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
}
```

```{r}
rbinom(25, 10, 0.15)
```

Supose you have a sample that comes from a binomial distribution, and the sample size is igual to m. How can you estimate the parameter p and n With only that sample?

To solve this problem we use a estimator deriving from the likelihood function, which is define as:

\begin{equation}
  L(x; \theta) = \prod_{i = 1}^n f(x_i; \theta) \label{lik}
\end{equation}

And then we find the maximum point of the function, or in other words, the point that make the derivative of $f(x; \theta)$ igual to 0.

# The Problem

Let $X_1, X_2, \cdots, X_m$ be a random variable i.i.d. (Independent and identically distributed) a binomial sample of m observations with parameters n and p, in other words $X \sim Bin(n,p)$. Then we know that the density function of a binomial is:

\begin{equation}
  f(x; \{n, p\}) = \binom{n}{x} p^x (1 - p)^{n - x}; \qquad \text{$n \in \mathbb{Z}^* \quad p \in (0,1)$} \label{binom}
\end{equation}

So first of all we need to find the likehood function for a binomial distribution so using the equation \ref{lik}, where $f(x; \theta)$ for $\theta = \{ n, p \}$, expressed in the equation \ref{binom}.

## Estimation

Usually is more convenient to work with the derivative of $log[(L(x; \theta)]$, note that the maximum point of $L(x; \theta)$ and $log[L(x; \theta)]$ is the same point. Since we are looking at the maximum point then we know that $L'(x; \theta) = 0$, so:

\begin{align*}
  log'[L(x; \theta)] =& \frac{\partial log[L(x; \theta)]}{\partial \theta}  \\
  =& \frac{1}{L(x; \theta)} \frac{\partial L(x; \theta)}{\partial \theta}; \quad \text{but we know that in the maximum point} \frac{\partial L(x; \theta)}{\partial \theta} = 0 \\
  \Rightarrow \frac{\partial log[L(x; \theta)]}{\partial \theta} =& 0 
\end{align*}

$\forall x_0 \quad$That $f(x_0; \theta) \neq 0$.

\begin{align*}
  L(x; \theta) =& \prod_{i = 1}^n f(x_i; \theta) \\
  =& \prod_{i = 1}^n \binom{n}{x_i} p^{x_i} (1 - p)^{n - x_i} \\
  \Rightarrow l(x; \theta) =& \sum_{i = 1}^m log \Big{[} \binom{n}{x_i} \Big{]} + \sum_{i = 1}^m x_i log(p) + \sum_{i = 1}^m (n - x_i) log(1 - p)\\
  =& \sum_{i = 1}^m log \Big{[} \binom{n}{x_i} \Big{]} + m\bar{X}log(p) + (mn - m\bar{X})log(1 - p)
\end{align*}

## Estimate p, with n known

Supposing that the parameter n is know, in order to estimate the parameter p we can derivate $l(x; \theta)$ in relation to p. We have:

\begin{align*}
  \frac{\partial l(x; \theta)}{\partial p} =& \frac{\partial}{\partial p}\Big{[} \sum_{i = 1}^m log \Big{[} \binom{n}{x_i} \Big{]} + m\bar{X}log(p) + (mn - m\bar{X})log(1 - p) \Big{]} \\
  \Rightarrow& \frac{m\bar{X}}{\hat{p}} - \frac{n - \bar{X}}{1 - \hat{p}} =& 0
\end{align*}

There's no explict formular for the estimator of p, so we need to use some computation way to find the maximum root of the derivative of $l(x; \theta)$. A very good way of doing it is know as the Newton-Raphoson Method, there're mode way to solve the problem, such as the secant method, Bisection method or even finding the maximum point of the likehood function (which will be done in the last section). In this document we will use the Newton-Raphson method.

## Newton-Raphson method

Newton-Raphson method is an interactive that start with a initial guess of the root. The Newton-Raphson method is an approch to find the roots of non-linear equations, is a well-known and widely used for his simplicity and his speed for convergency.

In this document we will not explain the thery behind the method, but the image bellow is a pretty good way to have a hint on how it works. Basically, given a start point (first guess), is calculated the derivative, so is found the point the the tangent line "touch" the x-axis, then is calculated his image on y-axis and the the derivative on the point, and so on. Until in one time the difference on the two point is lower than the tolerance accepted for the user.

```{r, include = T, fig.align = 'center'}
image_read('https://predictivehacks.com/wp-content/uploads/2020/08/newtonRaphsonMethod.png')
```

\begin{equation}
  x_{n + 1} = x_n - \frac{f(x_n)}{f'(x_n)}
\end{equation}

The code used to find the root using the Newton-Raphson method were made by Aaron Schlegel on the \href{https://rpubs.com/aaronsc32/newton-raphson-method}{LINK}, as it follows:

```{r, include = T, echo = T}
## Newton-Raphson

newton.raphson <- function(f, a, b, tol = 1e-5, n = 1000) {
  require(numDeriv) # Package for computing f'(x)
  
  x0 <- a # Set start value to supplied lower bound
  k <- n # Initialize for iteration results
  
  # Check the upper and lower bounds to see if approximations result in 0
  fa <- f(a)
  if (fa == 0.0) {
    return(a)
  }
  
  fb <- f(b)
  if (fb == 0.0) {
    return(b)
  }
  
  for (i in 1:n) {
    dx <- genD(func = f, x = x0)$D[1] # First-order derivative f'(x0)
    x1 <- x0 - (f(x0) / dx) # Calculate next value x1
    k[i] <- x1 # Store x1
    # Once the difference between x0 and x1 becomes sufficiently small, output the results.
    if (abs(x1 - x0) < tol) {
      root.approx <- tail(k, n=1)
      res <- list('root approximation' = root.approx, 'iterations' = k)
      return(res)
    }
    # If Newton-Raphson has not yet reached convergence set x1 as x0 and continue
    x0 <- x1
  }
  print('Too many iterations in method')
}
```

```{r}
# Estimar p => n Conhecido

n <- 17
frp <- function(x) 1/print(x)*media - (n - media)/(1-x)

p_hat <- newton.raphson(frp, 0.001, 1) %>% 
  quiet()
p_hat

# param_p <- matrix(NA, ncol = 2, nrow = 10000)
# for (i in 1:10000) {
#   set.seed(18122021-i)
#   data <- rbinom(275, 17, 0.75)
# 
#   m <- length(data)
#   media <- mean(data)
# 
#   val <- newton.raphson(frp, 0.001, 1) %>% 
#   quiet() %>% 
#     .$`root approximation`
#   param_p[i,] <- cbind(i, val)
# }
# 
# param_p[,2] %>%
#   round(4) %>%
#   as_tibble() %>%
#   mutate(ID = 1:length(value)) %>%
#   ggplot(aes(x = ID, y = value))+
#   geom_point()+
#   geom_hline(yintercept = 0.75, color = 'red')+
#   theme_classic()
```


## Estimate n, with p know

```{r}
# Estimar n => p Conhecido

set.seed(18122021)
p <- 0.15
data <- rbinom(275, 17, p)
m <- length(data)
media <- mean(data)

frn <- function(x) m*psigamma((x+1),0) - sum(psigamma((x - data + 1))) + m*log(1-p)

n_hat <- newton.raphson(frn,max(data),100) %>% 
  quiet()
n_hat$`root approximation`
```


## Both n and p unknow

```{r}
# n e p desconhecidos
## Maximizar verossimilhan√ßa

set.seed(18122021)
data <- rbinom(275, 17, 0.75)

m <- length(data)
media <- mean(data)

optim(
  fn = vero
  , par = c(0.01,(max(data)+1))
  , lower = c(0.001,max(data))
  , upper = c(0.999,100)
  , method = 'L-BFGS-B'
  , hessian = T
)
```

```{r}
# param <- matrix(NA, ncol = 2, nrow = 10000)
# for (i in 1:10000) {
#   data <- rbinom(275, 17, 0.75)
#   
#   m <- length(data)
#   media <- mean(data)
#   
#   val <- optim(
#     fn = vero
#     , par = c(0.01,(max(data)+1))
#     , lower = c(0.001,max(data))
#     , upper = c(0.999,100)
#     , method = 'L-BFGS-B'
#     , hessian = T
#   )
#   param[i,] <- val$par
# }
# 
# param[,2] %>%
#   as_tibble() %>% 
#   mutate(ID = 1:length(value)) %>% 
#   ggplot(aes(x = ID, y = value))+
#   geom_point()+
#   geom_hline(yintercept = 17, color = 'red')+
#   theme_classic()
# 
# param[,1] %>%
#   round(4) %>% 
#   as_tibble() %>% 
#   mutate(ID = 1:length(value)) %>% 
#   ggplot(aes(x = ID, y = value))+
#   geom_point()+
#   geom_hline(yintercept = 0.75, color = 'red')+
#   theme_classic()
```
